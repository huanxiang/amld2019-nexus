{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a Data Pipeline for a Recommender System using Blue Brain Nexus \n",
    "\n",
    "In this notebook, you will create a recommendation engine using Blue Brain Nexus. Using some movie rating data,\n",
    "you will train a collaborative filtering model for movie recommendation using a given matrix factorization class and export the trained model to Elasticsearch. Once exported, \n",
    "you can test your recommendations by querying Elasticsearch and displaying the results.\n",
    "\n",
    "![Movie Recommendation](recommendation.png)\n",
    "\n",
    "### _Prerequisites_\n",
    "\n",
    "The notebook assumes you have installed Nexus SDK (https://github.com/BlueBrain/nexus-python-sdk).\n",
    "\n",
    "\n",
    "## Overview\n",
    "\n",
    "You will work through the following steps\n",
    "\n",
    "1. Set up the Nexus environment in Python \n",
    "2. Pull the data from Nexus\n",
    "3. Prepare the data\n",
    "4. Train the recommendation model\n",
    "5. Push the model results to Nexus\n",
    "6. Show recommendations by querying Nexus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Set up Nexus environment\n",
    "\n",
    "Blue Brain Nexus provides a Python SDK to facilitate the use of Nexus, including functionalities of authentication and data access, etc. The SDK is available at https://github.com/BlueBrain/nexus-python-sdk.\n",
    "\n",
    "Here, we will first pip install the sdk. Then, we will set up the Nexus environment in this notebook with a provided access token. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install git+https://github.com/BlueBrain/nexus-python-sdk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nexussdk as nexus\n",
    "\n",
    "deployment = 'YOUR ENVIRONMENT'\n",
    "token = 'YOUR ACCESS TOKEN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "nexus.config.set_environment(deployment)\n",
    "nexus.config.set_token(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Pull data from Nexus\n",
    "\n",
    "Now we will start pulling data that has been ingested into your Nexus project previously. \n",
    "\n",
    "For building a classical recommendation system using matrix factorization, we will need a user-by-item matrix where nonzero elements of the matrix are ratings that a user has given an item. To do that, we will \n",
    "\n",
    "1.   Query all the rating data from Nexus\n",
    "2.   Fetch each rating in a JSON format\n",
    "3.   Add each data into a DataFrame object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Prepare the data\n",
    "\n",
    "To get the recommendation right, we must construct and transform the data correctly. This is usually a very important step so that you are sure your machine learning algorithm is consuming the correct data in a good way.\n",
    "\n",
    "In the case of collaborative filtering using matrix factorization, the preparation of the data contains the following steps:\n",
    "\n",
    "- As in the U-I matrix we will have the user id and the item id as incremental integers, we will assign a unique number between (0, #users) to each user and do the same for movies. The mapping between the id in the U-I matrix will be stored, which can be further used to query the recommendation. \n",
    "\n",
    "\n",
    "- Then, we will create the U-I matrix by assigning each user's rating to each movie on a zero matrix created using numpy.\n",
    "\n",
    "\n",
    "- Finally, we will split the data into training and testing. This is done by removing 10 ratings for each user and assign them to the test set. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from numpy.linalg import solve\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['user_id', 'item_id', 'rating', 'timestamp']\n",
    "df = pd.read_csv('../data/ml-latest-small/ratings.csv', names=names)\n",
    "df = df.drop(0).astype(np.float)\n",
    "\n",
    "item_mapping = dict( enumerate(df.item_id.astype('category').cat.categories) )\n",
    "inv_item_mapping = {v: k for k, v in item_mapping.items()}\n",
    "\n",
    "df.user_id = df.user_id.astype('category').cat.codes.values\n",
    "df.item_id = df.item_id.astype('category').cat.codes.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_to_movie = {}\n",
    "movie_df = pd.read_csv('/Users/hulu/Downloads/ml-latest-small/movies.csv')\n",
    "for k, v in item_mapping.items():\n",
    "    idx_to_movie[k] = movie_df[movie_df.movieId==v].title.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_users = df.user_id.unique().shape[0]\n",
    "n_items = df.item_id.unique().shape[0]\n",
    "\n",
    "# Create r_{ui}, our ratings matrix\n",
    "ratings = np.zeros((n_users, n_items))\n",
    "for row in df.itertuples():\n",
    "    ratings[row[1]-1, row[2]-1] = row[3]\n",
    "\n",
    "# Split into training and test sets. \n",
    "# Remove 10 ratings for each user \n",
    "# and assign them to the test set\n",
    "def train_test_split(ratings):\n",
    "    test = np.zeros(ratings.shape)\n",
    "    train = ratings.copy()\n",
    "    for user in range(ratings.shape[0]):\n",
    "        test_ratings = np.random.choice(ratings[user, :].nonzero()[0], \n",
    "                                        size=10, \n",
    "                                        replace=False)\n",
    "        train[user, test_ratings] = 0.\n",
    "        test[user, test_ratings] = ratings[user, test_ratings]\n",
    "        \n",
    "    # Test and training are truly disjoint\n",
    "    assert(np.all((train * test) == 0)) \n",
    "    return train, test\n",
    "\n",
    "train, test = train_test_split(ratings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Train a recommmender model on the ratings data\n",
    "\n",
    "Your data is now prepared as a U-I matrix and you will use it to build a collaborative filtering recommendation model.\n",
    "\n",
    "Collaborative filtering is a recommendation approach that is effectively based on the \"wisdom of the crowd\". It makes the assumption that, if two people share similar preferences, then the things that one of them prefers could be good recommendations to make to the other. In other words, if user A tends to like certain movies, and user B shares some of these preferences with user A, then the movies that user A likes, that user B has not yet seen, may well be movies that user B will also like.\n",
    "\n",
    "In a similar manner, we can think about items as being similar if they tend to be rated highly by the same people, on average.\n",
    "\n",
    "Hence these models are based on the combined, collaborative preferences and behavior of all users in aggregate. They tend to be very effective in practice (provided you have enough preference data to train the model). The ratings data you have is a form of explicit preference data, perfect for training collaborative filtering models. \n",
    "\n",
    "Matrix factorization (MF) is a classical method to perform collaborative filtering model. The core idea of MF is to represent the ratings as a user-item ratings matrix. In the diagram below you will see this matrix on the left (with users as rows and movies as columns). The entries in this matrix are the ratings given by users to movies. \n",
    "\n",
    "You may also notice that the matrix has missing entries because not all users have rated all movies. In this situation we refer to the data as sparse.\n",
    "\n",
    "![alt text]()\n",
    "\n",
    "MF methods aim to find two much smaller matrices (one representing the users and the other the items) that, when multiplied together, re-construct the original ratings matrix as closely as possible. This is know as factorizing the original matrix, hence the name of the technique.\n",
    "\n",
    "The two smaller matrices are called factor matrices (or latent features). The user and movie factor matrices are illustrated on the right in the diagram above. The idea is that each user factor vector is a compressed representation of the user's preferences and behavior. Likewise, each item factor vector is a compressed representation of the item. Once the model is trained, the factor vectors can be used to make recommendations, which is what you will do in the following sections.\n",
    "\n",
    "The optimization of the MF can be done using different methods. In this example, we will use 2 popular methods:\n",
    "\n",
    "- Alternating Least Squares (ALS)\n",
    "\n",
    "- Stochastic Gradient Descent (SGD)\n",
    "\n",
    "\n",
    "Further reading:\n",
    "\n",
    "[Explicit Matrix Factorization: ALS, SGD, and All That Jazz](https://www.ethanrosenthal.com/2016/01/09/explicit-matrix-factorization-sgd-als/)\n",
    "\n",
    "[ALS Implicit Collaborative Filtering\n",
    "](https://medium.com/radon-dev/als-implicit-collaborative-filtering-5ed653ba39fe)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we have provided an explicit MF class which can perform MF with both ALS and SGD optimizations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExplicitMF():\n",
    "    def __init__(self, \n",
    "                 ratings,\n",
    "                 n_factors=40,\n",
    "                 learning='sgd',\n",
    "                 item_fact_reg=0.0, \n",
    "                 user_fact_reg=0.0,\n",
    "                 item_bias_reg=0.0,\n",
    "                 user_bias_reg=0.0,\n",
    "                 verbose=False):\n",
    "        \"\"\"\n",
    "        Train a matrix factorization model to predict empty \n",
    "        entries in a matrix. The terminology assumes a \n",
    "        ratings matrix which is ~ user x item\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "        ratings : (ndarray)\n",
    "            User x Item matrix with corresponding ratings\n",
    "        \n",
    "        n_factors : (int)\n",
    "            Number of latent factors to use in matrix \n",
    "            factorization model\n",
    "        learning : (str)\n",
    "            Method of optimization. Options include \n",
    "            'sgd' or 'als'.\n",
    "        \n",
    "        item_fact_reg : (float)\n",
    "            Regularization term for item latent factors\n",
    "        \n",
    "        user_fact_reg : (float)\n",
    "            Regularization term for user latent factors\n",
    "            \n",
    "        item_bias_reg : (float)\n",
    "            Regularization term for item biases\n",
    "        \n",
    "        user_bias_reg : (float)\n",
    "            Regularization term for user biases\n",
    "        \n",
    "        verbose : (bool)\n",
    "            Whether or not to printout training progress\n",
    "        \"\"\"\n",
    "        \n",
    "        self.ratings = ratings\n",
    "        self.n_users, self.n_items = ratings.shape\n",
    "        self.n_factors = n_factors\n",
    "        self.item_fact_reg = item_fact_reg\n",
    "        self.user_fact_reg = user_fact_reg\n",
    "        self.item_bias_reg = item_bias_reg\n",
    "        self.user_bias_reg = user_bias_reg\n",
    "        self.learning = learning\n",
    "        if self.learning == 'sgd':\n",
    "            self.sample_row, self.sample_col = self.ratings.nonzero()\n",
    "            self.n_samples = len(self.sample_row)\n",
    "        self._v = verbose\n",
    "\n",
    "    def als_step(self,\n",
    "                 latent_vectors,\n",
    "                 fixed_vecs,\n",
    "                 ratings,\n",
    "                 _lambda,\n",
    "                 type='user'):\n",
    "        \"\"\"\n",
    "        One of the two ALS steps. Solve for the latent vectors\n",
    "        specified by type.\n",
    "        \"\"\"\n",
    "        if type == 'user':\n",
    "            # Precompute\n",
    "            YTY = fixed_vecs.T.dot(fixed_vecs)\n",
    "            lambdaI = np.eye(YTY.shape[0]) * _lambda\n",
    "\n",
    "            for u in range(latent_vectors.shape[0]):\n",
    "                latent_vectors[u, :] = solve((YTY + lambdaI), \n",
    "                                             ratings[u, :].dot(fixed_vecs))\n",
    "        elif type == 'item':\n",
    "            # Precompute\n",
    "            XTX = fixed_vecs.T.dot(fixed_vecs)\n",
    "            lambdaI = np.eye(XTX.shape[0]) * _lambda\n",
    "            \n",
    "            for i in range(latent_vectors.shape[0]):\n",
    "                latent_vectors[i, :] = solve((XTX + lambdaI), \n",
    "                                             ratings[:, i].T.dot(fixed_vecs))\n",
    "        return latent_vectors\n",
    "\n",
    "    def train(self, n_iter=10, learning_rate=0.1):\n",
    "        \"\"\" Train model for n_iter iterations from scratch.\"\"\"\n",
    "        # initialize latent vectors        \n",
    "        self.user_vecs = np.random.normal(scale=1./self.n_factors,\\\n",
    "                                          size=(self.n_users, self.n_factors))\n",
    "        self.item_vecs = np.random.normal(scale=1./self.n_factors,\n",
    "                                          size=(self.n_items, self.n_factors))\n",
    "        \n",
    "        if self.learning == 'als':\n",
    "            self.partial_train(n_iter)\n",
    "        elif self.learning == 'sgd':\n",
    "            self.learning_rate = learning_rate\n",
    "            self.user_bias = np.zeros(self.n_users)\n",
    "            self.item_bias = np.zeros(self.n_items)\n",
    "            self.global_bias = np.mean(self.ratings[np.where(self.ratings != 0)])\n",
    "            self.partial_train(n_iter)\n",
    "    \n",
    "    \n",
    "    def partial_train(self, n_iter):\n",
    "        \"\"\" \n",
    "        Train model for n_iter iterations. Can be \n",
    "        called multiple times for further training.\n",
    "        \"\"\"\n",
    "        ctr = 1\n",
    "        while ctr <= n_iter:\n",
    "            if ctr % 10 == 0 and self._v:\n",
    "                print ('\\tcurrent iteration: {}'.format(ctr))\n",
    "            if self.learning == 'als':\n",
    "                self.user_vecs = self.als_step(self.user_vecs, \n",
    "                                               self.item_vecs, \n",
    "                                               self.ratings, \n",
    "                                               self.user_fact_reg, \n",
    "                                               type='user')\n",
    "                self.item_vecs = self.als_step(self.item_vecs, \n",
    "                                               self.user_vecs, \n",
    "                                               self.ratings, \n",
    "                                               self.item_fact_reg, \n",
    "                                               type='item')\n",
    "            elif self.learning == 'sgd':\n",
    "                self.training_indices = np.arange(self.n_samples)\n",
    "                np.random.shuffle(self.training_indices)\n",
    "                self.sgd()\n",
    "            ctr += 1\n",
    "\n",
    "    def sgd(self):\n",
    "        for idx in self.training_indices:\n",
    "            u = self.sample_row[idx]\n",
    "            i = self.sample_col[idx]\n",
    "            prediction = self.predict(u, i)\n",
    "            e = (self.ratings[u,i] - prediction) # error\n",
    "            \n",
    "            # Update biases\n",
    "            self.user_bias[u] += self.learning_rate * \\\n",
    "                                (e - self.user_bias_reg * self.user_bias[u])\n",
    "            self.item_bias[i] += self.learning_rate * \\\n",
    "                                (e - self.item_bias_reg * self.item_bias[i])\n",
    "            \n",
    "            #Update latent factors\n",
    "            self.user_vecs[u, :] += self.learning_rate * \\\n",
    "                                    (e * self.item_vecs[i, :] - \\\n",
    "                                     self.user_fact_reg * self.user_vecs[u,:])\n",
    "            self.item_vecs[i, :] += self.learning_rate * \\\n",
    "                                    (e * self.user_vecs[u, :] - \\\n",
    "                                     self.item_fact_reg * self.item_vecs[i,:])\n",
    "    def predict(self, u, i):\n",
    "        \"\"\" Single user and item prediction.\"\"\"\n",
    "        if self.learning == 'als':\n",
    "            return self.user_vecs[u, :].dot(self.item_vecs[i, :].T)\n",
    "        elif self.learning == 'sgd':\n",
    "            prediction = self.global_bias + self.user_bias[u] + self.item_bias[i]\n",
    "            prediction += self.user_vecs[u, :].dot(self.item_vecs[i, :].T)\n",
    "            return prediction\n",
    "    \n",
    "    def predict_all(self):\n",
    "        \"\"\" Predict ratings for every user and item.\"\"\"\n",
    "        predictions = np.zeros((self.user_vecs.shape[0], \n",
    "                                self.item_vecs.shape[0]))\n",
    "        for u in range(self.user_vecs.shape[0]):\n",
    "            for i in range(self.item_vecs.shape[0]):\n",
    "                predictions[u, i] = self.predict(u, i)\n",
    "                \n",
    "        return predictions\n",
    "    \n",
    "    def calculate_learning_curve(self, iter_array, test, learning_rate=0.1):\n",
    "        \"\"\"\n",
    "        Keep track of MSE as a function of training iterations.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "        iter_array : (list)\n",
    "            List of numbers of iterations to train for each step of \n",
    "            the learning curve. e.g. [1, 5, 10, 20]\n",
    "        test : (2D ndarray)\n",
    "            Testing dataset (assumed to be user x item).\n",
    "        \n",
    "        The function creates two new class attributes:\n",
    "        \n",
    "        train_mse : (list)\n",
    "            Training data MSE values for each value of iter_array\n",
    "        test_mse : (list)\n",
    "            Test data MSE values for each value of iter_array\n",
    "        \"\"\"\n",
    "        iter_array.sort()\n",
    "        self.train_mse =[]\n",
    "        self.test_mse = []\n",
    "        iter_diff = 0\n",
    "        for (i, n_iter) in enumerate(iter_array):\n",
    "            if self._v:\n",
    "                print ('Iteration: {}'.format(n_iter))\n",
    "            if i == 0:\n",
    "                self.train(n_iter - iter_diff, learning_rate)\n",
    "            else:\n",
    "                self.partial_train(n_iter - iter_diff)\n",
    "\n",
    "            predictions = self.predict_all()\n",
    "\n",
    "            self.train_mse += [get_mse(predictions, self.ratings)]\n",
    "            self.test_mse += [get_mse(predictions, test)]\n",
    "            if self._v:\n",
    "                print ('Train mse: ' + str(self.train_mse[-1]))\n",
    "                print ('Test mse: ' + str(self.test_mse[-1]))\n",
    "            iter_diff = n_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_als_model = ExplicitMF(ratings, n_factors=20, learning='als', \\\n",
    "                            item_fact_reg=0.01, user_fact_reg=0.01)\n",
    "best_als_model.train(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_sgd_model = ExplicitMF(ratings, n_factors=80, learning='sgd', \\\n",
    "                            item_fact_reg=0.01, user_fact_reg=0.01, \\\n",
    "                            user_bias_reg=0.01, item_bias_reg=0.01)\n",
    "best_sgd_model.train(200, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Push the embedding matrix to Nexus\n",
    "\n",
    "Now that we have the models trained, we can now push the embedding matrices back to Nexus and have them indexed in the  ElasticSearch view. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Show recommendation by querying Nexus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(model):\n",
    "    sim = model.item_vecs.dot(model.item_vecs.T)\n",
    "    norms = np.array([np.sqrt(np.diagonal(sim))])\n",
    "    return sim / norms / norms.T\n",
    "\n",
    "als_sim = cosine_similarity(best_als_model)\n",
    "sgd_sim = cosine_similarity(best_sgd_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_top_k_movies_name(similarity, mapper, movie_idx, k=5):\n",
    "    print('The recommended films for user who likes \"%s\"' % (mapper[movie_idx]))\n",
    "    \n",
    "    movie_indices = np.argsort(similarity[movie_idx,:])[::-1]\n",
    "    images = ''\n",
    "    k_ctr = 0\n",
    "    # Start i at 1 to not grab the input movie\n",
    "    i = 1\n",
    "    while k_ctr < 5:\n",
    "        movie = mapper[movie_indices[i]]\n",
    "        print(' - ' + movie)\n",
    "        k_ctr += 1\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The recommended films for user who likes \"Heat (1995)\"\n",
      " - Up Close and Personal (1996)\n",
      " - Last Supper, The (1995)\n",
      " - Malice (1993)\n",
      " - Speed (1994)\n",
      " - Eye for an Eye (1996)\n"
     ]
    }
   ],
   "source": [
    "movie_id = 5 # Heat\n",
    "display_top_k_movies_name(als_sim, idx_to_movie, movie_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Factors: 5\n",
      "Regularization: 0.001\n",
      "New optimal hyperparameters\n",
      "model        <__main__.ExplicitMF object at 0x1a0a196a58>\n",
      "n_factors                                               5\n",
      "n_iter                                                  5\n",
      "reg                                                 0.001\n",
      "test_mse                                           10.851\n",
      "train_mse                                         7.37505\n",
      "dtype: object\n",
      "Regularization: 0.01\n",
      "Regularization: 0.1\n",
      "Regularization: 1.0\n",
      "Factors: 10\n",
      "Regularization: 0.001\n",
      "New optimal hyperparameters\n",
      "model        <__main__.ExplicitMF object at 0x1a0a1969b0>\n",
      "n_factors                                              10\n",
      "n_iter                                                 25\n",
      "reg                                                 0.001\n",
      "test_mse                                          10.4631\n",
      "train_mse                                         6.50026\n",
      "dtype: object\n",
      "Regularization: 0.01\n",
      "Regularization: 0.1\n",
      "Regularization: 1.0\n",
      "Factors: 20\n",
      "Regularization: 0.001\n",
      "New optimal hyperparameters\n",
      "model        <__main__.ExplicitMF object at 0x1a0a196ac8>\n",
      "n_factors                                              20\n",
      "n_iter                                                 10\n",
      "reg                                                 0.001\n",
      "test_mse                                          10.3322\n",
      "train_mse                                         5.56849\n",
      "dtype: object\n",
      "Regularization: 0.01\n",
      "Regularization: 0.1\n",
      "Regularization: 1.0\n",
      "Factors: 40\n",
      "Regularization: 0.001\n",
      "New optimal hyperparameters\n",
      "model        <__main__.ExplicitMF object at 0x1a0a196ba8>\n",
      "n_factors                                              40\n",
      "n_iter                                                 50\n",
      "reg                                                 0.001\n",
      "test_mse                                          10.2539\n",
      "train_mse                                          4.3877\n",
      "dtype: object\n",
      "Regularization: 0.01\n",
      "Regularization: 0.1\n",
      "Regularization: 1.0\n",
      "Factors: 80\n",
      "Regularization: 0.001\n",
      "Regularization: 0.01\n",
      "Regularization: 0.1\n",
      "Regularization: 1.0\n"
     ]
    }
   ],
   "source": [
    "iter_array = [1, 2, 5, 10, 25, 50, 100, 200]\n",
    "latent_factors = [5, 10, 20, 40, 80]\n",
    "regularizations = [0.001, 0.01, 0.1, 1.]\n",
    "regularizations.sort()\n",
    "\n",
    "best_params = {}\n",
    "best_params['n_factors'] = latent_factors[0]\n",
    "best_params['reg'] = regularizations[0]\n",
    "best_params['n_iter'] = 0\n",
    "best_params['train_mse'] = np.inf\n",
    "best_params['test_mse'] = np.inf\n",
    "best_params['model'] = None\n",
    "\n",
    "for fact in latent_factors:\n",
    "    print ('Factors: {}'.format(fact))\n",
    "    for reg in regularizations:\n",
    "        print ('Regularization: {}'.format(reg))\n",
    "        MF_SGD = ExplicitMF(train, n_factors=fact, learning='als',\\\n",
    "                            user_fact_reg=reg, item_fact_reg=reg, \\\n",
    "                            user_bias_reg=reg, item_bias_reg=reg)\n",
    "        MF_SGD.calculate_learning_curve(iter_array, test, learning_rate=0.001)\n",
    "        min_idx = np.argmin(MF_SGD.test_mse)\n",
    "        if MF_SGD.test_mse[min_idx] < best_params['test_mse']:\n",
    "            best_params['n_factors'] = fact\n",
    "            best_params['reg'] = reg\n",
    "            best_params['n_iter'] = iter_array[min_idx]\n",
    "            best_params['train_mse'] = MF_SGD.train_mse[min_idx]\n",
    "            best_params['test_mse'] = MF_SGD.test_mse[min_idx]\n",
    "            best_params['model'] = MF_SGD\n",
    "            print ('New optimal hyperparameters')\n",
    "            print (pd.Series(best_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda]",
   "language": "python",
   "name": "conda-env-anaconda-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
